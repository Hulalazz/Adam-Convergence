{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Comparing *ADAM* and *AMSGrad* on MNIST\n",
    "\n",
    "Here, I tune and train logistic regression models, to recreate the empirical results of section 5 of the paper.\n",
    "\n",
    "As specified in the paper, I fix the parameter $\\beta_1$ at .99, and tune the learning rate $\\alpha$ and the hyperparameter $\\beta_2$ using a gridsearch, as done in the paper. \n",
    "\n",
    "Note that to fit a logistic regression model, I'm using a one-layer feedforward neural network (they're equivalent). This is so that I can use the nice tools (including the ADAM optimizer) already implemented in the deep learning framework Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load MNIST Dataset\n",
    "\n",
    "I've already created train and test splits for the MNIST dataset. They are conviniently stored as compressed numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_np_file(path, mode = \"rb\"):\n",
    "    with open(path, mode) as handle:\n",
    "        return(np.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = load_np_file(\"../data/MNIST/X_train.npy\") \n",
    "X_test = load_np_file(\"../data/MNIST/X_test.npy\") \n",
    "y_train = load_np_file(\"../data/MNIST/y_train.npy\") \n",
    "y_test = load_np_file(\"../data/MNIST/y_test.npy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# sanity check - did all the shapes get preserved? \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A framework for exhaustive gridsearch\n",
    "\n",
    "The hyperpameters that I'll need to tune by gridsearch are: \n",
    "\n",
    "- $\\beta_2$\n",
    "- $\\alpha$.\n",
    "\n",
    "To do so in a neat fashion, and make use of all my cores (CPU training :( ) , I'll use the `GridSearchCV` class from `sklearn`, with the `KerasClassifier` wrapper.\n",
    "\n",
    "The interface of this wrapper requres that I define a function that can be called with a set of hyperparameter options and create a `Sequential` model that can be compiled and trained. This is what I do here. \n",
    "\n",
    "Note the hyperparameters that I do not tune, as they are fixed by the authors:\n",
    "\n",
    "- $\\beta_1 = .9$\n",
    "- Discount rate: $\\alpha_t$ = $\\frac{\\alpha}{\\sqrt{t}}$\n",
    "- Batch size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that, when passed with hyperparameter options, returns a compiled model\n",
    "# Note that if `amsgrad = True`, the method in the paper is used.\n",
    "def create_model(lr=0.01, beta_2 = .99, amsgrad = False):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=784, activation='sigmoid'))\n",
    "    \"\"\"\n",
    "    Create a learning rate schedule, \n",
    "    so that alpha_t = alpha/sqrt(t), as specified in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = Adam(lr=lr, beta_2 = beta_2, amsgrad = amsgrad, decay = .14)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gridsearch: Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I specify the ranges I'll want to look over for $\\alpha$ and $\\beta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99   0.9915 0.993  0.9945 0.996  0.9975 0.999 ]\n"
     ]
    }
   ],
   "source": [
    "beta2_range = np.arange(.99, .999, .0015)\n",
    "print( beta2_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-05, 5e-05, 0.00025, 0.00125, 0.00625, 0.03125]\n"
     ]
    }
   ],
   "source": [
    "alpha_range = [.00001*5**i for i in range(6)]\n",
    "print (alpha_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(lr=alpha_range, beta_2=beta2_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.wrappers.scikit_learn.KerasClassifier at 0x1131d4240>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the gridsearch. I'll do 3-fold cross validation to choose the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_grid = GridSearchCV(estimator=adam_model, param_grid=param_grid, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Epoch 1/100\n",
      " - 2s - loss: 8.6858 - acc: 0.1332\n",
      "Epoch 2/100\n",
      " - 2s - loss: 8.7259 - acc: 0.1345\n",
      "Epoch 2/100\n",
      " - 2s - loss: 8.1767 - acc: 0.1352\n",
      "Epoch 2/100\n",
      " - 3s - loss: 8.7753 - acc: 0.1310\n",
      "Epoch 2/100\n",
      " - 2s - loss: 8.6256 - acc: 0.1334\n",
      "Epoch 3/100\n",
      " - 2s - loss: 8.6639 - acc: 0.1348\n",
      "Epoch 3/100\n",
      " - 2s - loss: 7.8773 - acc: 0.1357\n",
      "Epoch 3/100\n",
      " - 2s - loss: 8.7135 - acc: 0.1312\n",
      "Epoch 3/100\n",
      " - 1s - loss: 8.6019 - acc: 0.1334\n",
      "Epoch 4/100\n",
      " - 2s - loss: 8.6391 - acc: 0.1349\n",
      "Epoch 4/100\n",
      " - 2s - loss: 7.7596 - acc: 0.1361\n",
      "Epoch 4/100\n",
      " - 2s - loss: 8.6892 - acc: 0.1313\n",
      "Epoch 4/100\n",
      " - 2s - loss: 8.5866 - acc: 0.1333\n",
      "Epoch 5/100\n",
      " - 2s - loss: 8.6231 - acc: 0.1348\n",
      "Epoch 5/100\n",
      " - 2s - loss: 7.6837 - acc: 0.1361\n",
      "Epoch 5/100\n",
      " - 2s - loss: 8.6733 - acc: 0.1312\n",
      "Epoch 5/100\n",
      " - 2s - loss: 8.5752 - acc: 0.1334\n",
      "Epoch 6/100\n",
      " - 2s - loss: 8.6112 - acc: 0.1347\n",
      "Epoch 6/100\n",
      " - 2s - loss: 7.6264 - acc: 0.1363\n",
      "Epoch 6/100\n",
      " - 2s - loss: 8.6615 - acc: 0.1313\n",
      "Epoch 6/100\n",
      " - 2s - loss: 8.5660 - acc: 0.1333\n",
      "Epoch 7/100\n",
      " - 2s - loss: 8.6016 - acc: 0.1346\n",
      "Epoch 7/100\n",
      " - 2s - loss: 7.5801 - acc: 0.1366\n",
      "Epoch 7/100\n",
      " - 2s - loss: 8.6523 - acc: 0.1312\n",
      "Epoch 7/100\n",
      " - 2s - loss: 8.5583 - acc: 0.1333\n",
      "Epoch 8/100\n",
      " - 2s - loss: 8.5937 - acc: 0.1347\n",
      "Epoch 8/100\n",
      " - 2s - loss: 7.5417 - acc: 0.1366\n",
      "Epoch 8/100\n",
      " - 2s - loss: 8.6445 - acc: 0.1314\n",
      "Epoch 8/100\n",
      " - 2s - loss: 8.5518 - acc: 0.1333\n",
      "Epoch 9/100\n",
      " - 2s - loss: 8.5869 - acc: 0.1347\n",
      "Epoch 9/100\n",
      " - 2s - loss: 7.5085 - acc: 0.1366\n",
      "Epoch 9/100\n",
      " - 2s - loss: 8.6379 - acc: 0.1315\n",
      "Epoch 9/100\n",
      " - 2s - loss: 8.5460 - acc: 0.1335\n",
      "Epoch 10/100\n",
      " - 2s - loss: 8.5810 - acc: 0.1347\n",
      "Epoch 10/100\n",
      " - 2s - loss: 7.4795 - acc: 0.1367\n",
      "Epoch 10/100\n",
      " - 2s - loss: 8.6321 - acc: 0.1315\n",
      "Epoch 10/100\n",
      " - 2s - loss: 8.5409 - acc: 0.1334\n",
      "Epoch 11/100\n",
      " - 2s - loss: 8.5757 - acc: 0.1346\n",
      "Epoch 11/100\n",
      " - 2s - loss: 7.4538 - acc: 0.1368\n",
      "Epoch 11/100\n",
      " - 2s - loss: 8.6269 - acc: 0.1315\n",
      "Epoch 11/100\n",
      " - 2s - loss: 8.5363 - acc: 0.1333\n",
      "Epoch 12/100\n",
      " - 2s - loss: 8.5710 - acc: 0.1347\n",
      "Epoch 12/100\n",
      " - 2s - loss: 8.6223 - acc: 0.1314\n",
      "Epoch 12/100\n",
      " - 2s - loss: 7.4309 - acc: 0.1369\n",
      "Epoch 12/100\n",
      " - 2s - loss: 8.5321 - acc: 0.1334\n",
      "Epoch 13/100\n",
      " - 2s - loss: 8.5667 - acc: 0.1347\n",
      "Epoch 13/100\n",
      " - 2s - loss: 7.4099 - acc: 0.1369\n",
      "Epoch 13/100\n",
      " - 2s - loss: 8.6180 - acc: 0.1313\n",
      "Epoch 13/100\n",
      " - 2s - loss: 8.5283 - acc: 0.1333\n",
      "Epoch 14/100\n",
      " - 2s - loss: 8.5627 - acc: 0.1347\n",
      "Epoch 14/100\n",
      " - 2s - loss: 8.6141 - acc: 0.1313\n",
      " - 2s - loss: 7.3908 - acc: 0.1370\n",
      "Epoch 14/100\n",
      "Epoch 14/100\n",
      " - 2s - loss: 8.5248 - acc: 0.1333\n",
      "Epoch 15/100\n",
      " - 1s - loss: 8.5591 - acc: 0.1347\n",
      "Epoch 15/100\n",
      " - 2s - loss: 8.6106 - acc: 0.1313\n",
      "Epoch 15/100\n",
      " - 2s - loss: 7.3729 - acc: 0.1371\n",
      "Epoch 15/100\n",
      " - 2s - loss: 8.5215 - acc: 0.1333\n",
      "Epoch 16/100\n",
      " - 2s - loss: 8.5557 - acc: 0.1347\n",
      "Epoch 16/100\n",
      " - 2s - loss: 8.6074 - acc: 0.1314\n",
      " - 2s - loss: 7.3565 - acc: 0.1372\n",
      "Epoch 16/100\n",
      "Epoch 16/100\n",
      " - 2s - loss: 8.5184 - acc: 0.1334\n",
      "Epoch 17/100\n",
      " - 2s - loss: 8.5525 - acc: 0.1347\n",
      "Epoch 17/100\n",
      " - 1s - loss: 8.6043 - acc: 0.1314\n",
      "Epoch 17/100\n",
      " - 2s - loss: 7.3410 - acc: 0.1372\n",
      "Epoch 17/100\n",
      " - 2s - loss: 8.5155 - acc: 0.1333\n",
      "Epoch 18/100\n",
      " - 2s - loss: 8.5495 - acc: 0.1347\n",
      "Epoch 18/100\n",
      " - 2s - loss: 8.6015 - acc: 0.1313\n",
      "Epoch 18/100\n",
      " - 2s - loss: 7.3265 - acc: 0.1372\n",
      "Epoch 18/100\n",
      " - 2s - loss: 8.5128 - acc: 0.1333\n",
      "Epoch 19/100\n",
      " - 2s - loss: 8.5467 - acc: 0.1346\n",
      "Epoch 19/100\n",
      " - 2s - loss: 8.5988 - acc: 0.1313\n",
      "Epoch 19/100\n",
      " - 2s - loss: 7.3128 - acc: 0.1373\n",
      "Epoch 19/100\n",
      " - 2s - loss: 8.5103 - acc: 0.1333\n",
      "Epoch 20/100\n",
      " - 2s - loss: 8.5440 - acc: 0.1346\n",
      "Epoch 20/100\n",
      " - 2s - loss: 8.5962 - acc: 0.1313\n",
      "Epoch 20/100\n",
      " - 2s - loss: 7.2999 - acc: 0.1373\n",
      "Epoch 20/100\n",
      " - 2s - loss: 8.5078 - acc: 0.1333\n",
      "Epoch 21/100\n",
      " - 2s - loss: 8.5415 - acc: 0.1346\n",
      "Epoch 21/100\n",
      " - 2s - loss: 8.5938 - acc: 0.1313\n",
      "Epoch 21/100\n",
      " - 2s - loss: 7.2878 - acc: 0.1372\n",
      "Epoch 21/100\n",
      " - 2s - loss: 8.5055 - acc: 0.1334\n",
      "Epoch 22/100\n",
      " - 2s - loss: 8.5391 - acc: 0.1346\n",
      "Epoch 22/100\n",
      " - 2s - loss: 8.5915 - acc: 0.1314\n",
      "Epoch 22/100\n",
      " - 2s - loss: 7.2764 - acc: 0.1372\n",
      "Epoch 22/100\n",
      " - 2s - loss: 8.5033 - acc: 0.1334\n",
      "Epoch 23/100\n",
      " - 2s - loss: 8.5368 - acc: 0.1346\n",
      "Epoch 23/100\n",
      " - 2s - loss: 8.5893 - acc: 0.1313\n",
      "Epoch 23/100\n",
      " - 2s - loss: 7.2656 - acc: 0.1372\n",
      "Epoch 23/100\n",
      " - 2s - loss: 8.5012 - acc: 0.1333\n",
      "Epoch 24/100\n",
      " - 2s - loss: 8.5346 - acc: 0.1346\n",
      "Epoch 24/100\n",
      " - 2s - loss: 8.5872 - acc: 0.1313\n",
      "Epoch 24/100\n",
      " - 2s - loss: 7.2552 - acc: 0.1372\n",
      "Epoch 24/100\n",
      " - 2s - loss: 8.4991 - acc: 0.1333\n",
      "Epoch 25/100\n",
      " - 2s - loss: 8.5325 - acc: 0.1346\n",
      "Epoch 25/100\n",
      " - 2s - loss: 8.5851 - acc: 0.1313\n",
      "Epoch 25/100\n",
      " - 2s - loss: 7.2453 - acc: 0.1372\n",
      "Epoch 25/100\n",
      " - 2s - loss: 8.4972 - acc: 0.1333\n",
      "Epoch 26/100\n",
      " - 2s - loss: 8.5305 - acc: 0.1345\n",
      "Epoch 26/100\n",
      " - 2s - loss: 8.5832 - acc: 0.1313\n",
      "Epoch 26/100\n",
      " - 2s - loss: 7.2359 - acc: 0.1372\n",
      "Epoch 26/100\n",
      " - 2s - loss: 8.4953 - acc: 0.1333\n",
      "Epoch 27/100\n",
      " - 2s - loss: 8.5286 - acc: 0.1345\n",
      "Epoch 27/100\n",
      " - 2s - loss: 8.5813 - acc: 0.1312\n",
      "Epoch 27/100\n",
      " - 2s - loss: 7.2267 - acc: 0.1372\n",
      "Epoch 27/100\n",
      " - 2s - loss: 8.4934 - acc: 0.1333\n",
      "Epoch 28/100\n",
      " - 2s - loss: 8.5267 - acc: 0.1346\n",
      "Epoch 28/100\n",
      " - 2s - loss: 8.5795 - acc: 0.1312\n",
      "Epoch 28/100\n",
      " - 2s - loss: 7.2179 - acc: 0.1371\n",
      "Epoch 28/100\n",
      " - 2s - loss: 8.4917 - acc: 0.1333\n",
      "Epoch 29/100\n",
      " - 2s - loss: 8.5249 - acc: 0.1346\n",
      "Epoch 29/100\n",
      " - 2s - loss: 8.5777 - acc: 0.1312\n",
      "Epoch 29/100\n",
      " - 2s - loss: 7.2095 - acc: 0.1372\n",
      "Epoch 29/100\n",
      " - 2s - loss: 8.4900 - acc: 0.1333\n",
      "Epoch 30/100\n",
      " - 2s - loss: 8.5231 - acc: 0.1345\n",
      "Epoch 30/100\n",
      " - 2s - loss: 8.5761 - acc: 0.1311\n",
      "Epoch 30/100\n",
      " - 2s - loss: 7.2016 - acc: 0.1371\n",
      "Epoch 30/100\n",
      " - 2s - loss: 8.4883 - acc: 0.1333\n",
      "Epoch 31/100\n",
      " - 2s - loss: 8.5215 - acc: 0.1345\n",
      "Epoch 31/100\n",
      " - 2s - loss: 8.5745 - acc: 0.1311\n",
      "Epoch 31/100\n",
      " - 2s - loss: 7.1938 - acc: 0.1371\n",
      "Epoch 31/100\n",
      " - 2s - loss: 8.4868 - acc: 0.1333\n",
      "Epoch 32/100\n",
      " - 2s - loss: 8.5198 - acc: 0.1345\n",
      "Epoch 32/100\n",
      " - 2s - loss: 8.5729 - acc: 0.1312\n",
      "Epoch 32/100\n",
      " - 2s - loss: 7.1864 - acc: 0.1371\n",
      "Epoch 32/100\n",
      " - 2s - loss: 8.4853 - acc: 0.1333\n",
      "Epoch 33/100\n",
      " - 3s - loss: 8.5183 - acc: 0.1345\n",
      "Epoch 33/100\n",
      " - 2s - loss: 8.5714 - acc: 0.1312\n",
      "Epoch 33/100\n",
      " - 2s - loss: 7.1792 - acc: 0.1371\n",
      "Epoch 33/100\n",
      " - 2s - loss: 8.4838 - acc: 0.1333\n",
      "Epoch 34/100\n",
      " - 2s - loss: 8.5168 - acc: 0.1344\n",
      "Epoch 34/100\n",
      " - 2s - loss: 8.5700 - acc: 0.1312\n",
      "Epoch 34/100\n",
      " - 2s - loss: 7.1722 - acc: 0.1371\n",
      "Epoch 34/100\n",
      " - 2s - loss: 8.4824 - acc: 0.1333\n",
      "Epoch 35/100\n",
      " - 2s - loss: 8.5153 - acc: 0.1344\n",
      "Epoch 35/100\n",
      " - 2s - loss: 8.5686 - acc: 0.1312\n",
      "Epoch 35/100\n",
      " - 2s - loss: 7.1654 - acc: 0.1372\n",
      "Epoch 35/100\n",
      " - 2s - loss: 8.4811 - acc: 0.1333\n",
      "Epoch 36/100\n",
      " - 3s - loss: 8.5139 - acc: 0.1344\n",
      "Epoch 36/100\n",
      " - 2s - loss: 8.5673 - acc: 0.1312\n",
      "Epoch 36/100\n",
      " - 3s - loss: 7.1587 - acc: 0.1372\n",
      "Epoch 36/100\n",
      " - 2s - loss: 8.4798 - acc: 0.1333\n",
      "Epoch 37/100\n",
      " - 2s - loss: 8.5126 - acc: 0.1344\n",
      "Epoch 37/100\n",
      " - 2s - loss: 8.5661 - acc: 0.1311\n",
      "Epoch 37/100\n",
      " - 2s - loss: 7.1524 - acc: 0.1372\n",
      "Epoch 37/100\n",
      " - 2s - loss: 8.4785 - acc: 0.1333\n",
      "Epoch 38/100\n",
      " - 2s - loss: 8.5113 - acc: 0.1344\n",
      "Epoch 38/100\n",
      " - 2s - loss: 8.5648 - acc: 0.1311\n",
      "Epoch 38/100\n",
      " - 2s - loss: 7.1462 - acc: 0.1372\n"
     ]
    }
   ],
   "source": [
    "# adam_grid = GridSearchCV(estimator=adam_model, param_grid=param_grid, n_jobs=-1)\n",
    "adam_grid_result = adam_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
